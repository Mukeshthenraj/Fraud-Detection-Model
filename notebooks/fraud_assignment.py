# -*- coding: utf-8 -*-
"""fraud_assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnXF4qI21mZL9DU6QBf2EgC2c5reAs9e

---

_You are currently looking at **version 0.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the Jupyter Notebook FAQ course resource._

---
"""

import numpy as np
import pandas as pd

"""### Question 1
Import the data from `assets/fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?

*This function should return a float between 0 and 1.*
"""

def answer_one():
    # Step 1: Read the CSV file
    df = pd.read_csv('assets/fraud_data.csv')

    # Step 2: Calculate % of frauds
    fraud_percentage = df['Class'].mean()

    # Step 3: Return the answer
    return fraud_percentage
print(answer_one())



# Use X_train, X_test, y_train, y_test for all of the following questions
from sklearn.model_selection import train_test_split

df = pd.read_csv('assets/fraud_data.csv')

X = df.iloc[:,:-1]
y = df.iloc[:,-1]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

"""### Question 2

Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?

*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*
"""

def answer_two():
    from sklearn.dummy import DummyClassifier
    from sklearn.metrics import accuracy_score, recall_score

    # Step 1: Create dummy model
    dummy = DummyClassifier(strategy='most_frequent')

    # Step 2: Train it
    dummy.fit(X_train, y_train)

    # Step 3: Predict on test data
    y_dummy_pred = dummy.predict(X_test)

    # Step 4: Calculate accuracy and recall
    acc = accuracy_score(y_test, y_dummy_pred)
    rec = recall_score(y_test, y_dummy_pred)

    # Step 5: Return result as a tuple
    return (acc, rec)
print(answer_two())



"""### Question 3

Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?

*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*
"""

def answer_three():
    from sklearn.svm import SVC
    from sklearn.metrics import accuracy_score, recall_score, precision_score

    # Step 1: Create the model with default settings
    model = SVC()

    # Step 2: Train it on training data
    model.fit(X_train, y_train)

    # Step 3: Predict on test data
    y_pred = model.predict(X_test)

    # Step 4: Calculate the metrics
    acc = accuracy_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)

    # Step 5: Return all three scores
    return (acc, rec, prec)
print(answer_three())



"""### Question 4

Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.

*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*
"""

def answer_four():
    from sklearn.svm import SVC
    from sklearn.metrics import confusion_matrix
    import numpy as np

    # Step 1: Train model with given parameters
    model = SVC(C=1e9, gamma=1e-07)
    model.fit(X_train, y_train)

    # Step 2: Get decision function scores
    scores = model.decision_function(X_test)

    # Step 3: Apply threshold of -220
    y_pred = (scores > -220).astype(int)

    # Step 4: Build confusion matrix
    cm = confusion_matrix(y_test, y_pred)

    return cm
print(answer_four())



"""### Question 5

Train a logisitic regression classifier with default parameters using X_train and y_train. This classifier should use the parameter solver='liblinear'.

For the logisitic regression classifier, compute the scores using decision_function() or with predict_proba(), then create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).

Looking at the precision recall curve, what is the recall when the precision is `0.75`?

Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?

Note: When getting the ROC curve and finding the records where the FPR entry is closest to 0.16, take the corresponding TPRs. As there are two such records where the FPR is close to 0.16, take the higher TPR of these two records.

*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*
"""

def answer_five():
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import precision_recall_curve, roc_curve
    import numpy as np

    # Step 1: Train the model
    model = LogisticRegression(solver='liblinear')
    model.fit(X_train, y_train)

    # Step 2: Get decision function scores
    scores = model.decision_function(X_test)

    # Step 3: Precision-Recall curve
    precision, recall, _ = precision_recall_curve(y_test, scores)

    # Find recall when precision is closest to 0.75
    pr_index = np.where(precision >= 0.75)[0][0]
    recall_at_075_precision = recall[pr_index]

    # Step 4: ROC curve
    fpr, tpr, _ = roc_curve(y_test, scores)

    # Find TPR when FPR is closest to 0.16
    fpr_diff = np.abs(fpr - 0.16)
    closest_index = np.where(fpr_diff == np.min(fpr_diff))[0]
    tpr_at_016_fpr = np.max(tpr[closest_index])  # Take the higher of the two

    return (recall_at_075_precision, tpr_at_016_fpr)
print(answer_five())





"""### Question 6

Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation. (Suggest to use `solver='liblinear'`, more explanation [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html))

`'penalty': ['l1', 'l2']`

`'C':[0.01, 0.1, 1, 10]`

From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.

|      	| `l1` 	| `l2` 	|
|:----:	|----	|----	|
| **`0.01`** 	|    ?	|   ? 	|
| **`0.1`**  	|    ?	|   ? 	|
| **`1`**    	|    ?	|   ? 	|
| **`10`**   	|    ?	|   ? 	|

<br>

*This function should return a 4 by 2 numpy array with 8 floats.*

*Note: do not return a DataFrame, just the values denoted by `?` in a numpy array.*
"""

def answer_six():
    from sklearn.model_selection import GridSearchCV
    from sklearn.linear_model import LogisticRegression
    import numpy as np

    # Step 1: Define the model
    model = LogisticRegression(solver='liblinear')

    # Step 2: Set up the parameter grid
    parameters = {
        'penalty': ['l1', 'l2'],
        'C': [0.01, 0.1, 1, 10]
    }

    # Step 3: Set up GridSearch with recall scoring
    grid = GridSearchCV(model, param_grid=parameters, scoring='recall', cv=3)
    grid.fit(X_train, y_train)

    # Step 4: Extract mean test scores from results
    results = grid.cv_results_['mean_test_score']

    # Step 5: Reshape into 4x2 array (4 C values Ã— 2 penalty types)
    return np.array(results).reshape(4, 2)
print(answer_six())



# Commented out IPython magic to ensure Python compatibility.
# Use the following function to help visualize results from the grid search
def GridSearch_Heatmap(scores):
    import matplotlib.pyplot as plt
    import seaborn as sns
    import numpy as np

    # Ensure plotting works in Coursera notebooks
#     %matplotlib inline

    # Plot the heatmap
    plt.figure(figsize=(6,4))
    sns.heatmap(scores.reshape(4, 2),
                annot=True, fmt=".3f", cmap="viridis",
                xticklabels=['l1', 'l2'],
                yticklabels=[0.01, 0.1, 1, 10])
    plt.xlabel('Penalty')
    plt.ylabel('C')
    plt.title('Grid Search Recall Scores')
    plt.yticks(rotation=0)
    plt.show()

GridSearch_Heatmap(answer_six())

